{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of dilepton final state from ATLAS OpenData 13TeV dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <iostream>\n",
    "#include <fstream>\n",
    "#include <algorithm>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of OpenData (13TeV):\n",
    "- Data is stored in nTuples, i.e. 'trees'\n",
    "- One event = one entry in the tree\n",
    "- One variable = one 'branch' in the tree (branch = integers/floats/vectors/booleans etc.)\n",
    "- All events have the same branches\n",
    "\n",
    "\n",
    "TChain is used in order to link together data from several nTuples, i.e. it is a 'chain of trees'.\n",
    "Would typically like one chain for real data and one for MC (background)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "\n",
    "1. Reading the dataset and obtaining header variables\n",
    "\n",
    "*/\n",
    "\n",
    "TChain *background = new TChain(\"mini\");\n",
    "TChain *data = new TChain(\"mini\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TString sample, path, type;\n",
    "Int_t DSID, mass;\n",
    "vector<TString> types;\n",
    "vector<Int_t> dataset_IDs, masses;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// dataset_MCInput_test2.txt contains only Z->uu, Z->tautau, W->unu, ttbar, dibosons and G->uu\n",
    "TString filename = \"../Input/13TeV/dataset_MCInput.txt\";\n",
    "ifstream infile1(filename);\n",
    "\n",
    "string name ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mc_305568.Gmumu_01_750.2lep.root\n",
      "305568 Graviton 750\n",
      "mc_305571.Gmumu_01_1000.2lep.root\n",
      "305571 Graviton 1000\n",
      "mc_305574.Gmumu_01_2000.2lep.root\n",
      "305574 Graviton 2000\n",
      "mc_305577.Gmumu_01_3000.2lep.root\n",
      "305577 Graviton 3000\n",
      "mc_305580.Gmumu_01_4000.2lep.root\n",
      "305580 Graviton 4000\n",
      "mc_341122.ggH125_tautaull.2lep.root\n",
      "341122 Higgs 0\n",
      "mc_341155.VBFH125_tautaull.2lep.root\n",
      "341155 Higgs 0\n",
      "mc_341947.ZH125_ZZ4lep.2lep.root\n",
      "341947 Higgs 0\n",
      "mc_341964.WH125_ZZ4lep.2lep.root\n",
      "341964 Higgs 0\n",
      "mc_344235.VBFH125_ZZ4lep.2lep.root\n",
      "344235 Higgs 0\n",
      "mc_345060.ggH125_ZZ4lep.2lep.root\n",
      "345060 Higgs 0\n",
      "mc_345323.VBFH125_WW2lep.2lep.root\n",
      "345323 Higgs 0\n",
      "mc_345324.ggH125_WW2lep.2lep.root\n",
      "345324 Higgs 0\n",
      "mc_345325.WpH125J_qqWW2lep.2lep.root\n",
      "345325 Higgs 0\n",
      "mc_345327.WpH125J_lvWW2lep.2lep.root\n",
      "345327 Higgs 0\n",
      "mc_345336.ZH125J_qqWW2lep.2lep.root\n",
      "345336 Higgs 0\n",
      "mc_345337.ZH125J_llWW2lep.2lep.root\n",
      "345337 Higgs 0\n",
      "mc_345445.ZH125J_vvWW2lep.2lep.root\n",
      "345445 Higgs 0\n",
      "mc_361101.Wplusmunu.2lep.root\n",
      "361101 W+jets 0\n",
      "mc_361102.Wplustaunu.2lep.root\n",
      "361102 W+jets 0\n",
      "mc_361104.Wminusmunu.2lep.root\n",
      "361104 W+jets 0\n",
      "mc_361105.Wminustaunu.2lep.root\n",
      "361105 W+jets 0\n",
      "mc_363356.ZqqZll.2lep.root\n",
      "363356 WZ 0\n",
      "mc_363358.WqqZll.2lep.root\n",
      "363358 WZ 0\n",
      "mc_363359.WpqqWmlv.2lep.root\n",
      "363359 WZ 0\n",
      "mc_363360.WplvWmqq.2lep.root\n",
      "363360 WZ 0\n",
      "mc_363489.WlvZqq.2lep.root\n",
      "363489 WZ 0\n",
      "mc_363490.llll.2lep.root\n",
      "363490 Dibosons 0\n",
      "mc_363491.lllv.2lep.root\n",
      "363491 Dibosons 0\n",
      "mc_363492.llvv.2lep.root\n",
      "363492 Dibosons 0\n",
      "mc_363493.lvvv.2lep.root\n",
      "363493 Dibosons 0\n",
      "mc_364100.Zmumu_PTV0_70_CVetoBVeto.2lep.root\n",
      "364100 Z+jets 0\n",
      "mc_364101.Zmumu_PTV0_70_CFilterBVeto.2lep.root\n",
      "364101 Z+jets 0\n",
      "mc_364102.Zmumu_PTV0_70_BFilter.2lep.root\n",
      "364102 Z+jets 0\n",
      "mc_364103.Zmumu_PTV70_140_CVetoBVeto.2lep.root\n",
      "364103 Z+jets 0\n",
      "mc_364104.Zmumu_PTV70_140_CFilterBVeto.2lep.root\n",
      "364104 Z+jets 0\n",
      "mc_364105.Zmumu_PTV70_140_BFilter.2lep.root\n",
      "364105 Z+jets 0\n",
      "mc_364106.Zmumu_PTV140_280_CVetoBVeto.2lep.root\n",
      "364106 Z+jets 0\n",
      "mc_364107.Zmumu_PTV140_280_CFilterBVeto.2lep.root\n",
      "364107 Z+jets 0\n",
      "mc_364108.Zmumu_PTV140_280_BFilter.2lep.root\n",
      "364108 Z+jets 0\n",
      "mc_364109.Zmumu_PTV280_500_CVetoBVeto.2lep.root\n",
      "364109 Z+jets 0\n",
      "mc_364110.Zmumu_PTV280_500_CFilterBVeto.2lep.root\n",
      "364110 Z+jets 0\n",
      "mc_364111.Zmumu_PTV280_500_BFilter.2lep.root\n",
      "364111 Z+jets 0\n",
      "mc_364112.Zmumu_PTV500_1000.2lep.root\n",
      "364112 Z+jets 0\n",
      "mc_364113.Zmumu_PTV1000_E_CMS.2lep.root\n",
      "364113 Z+jets 0\n",
      "mc_364128.Ztautau_PTV0_70_CVetoBVeto.2lep.root\n",
      "364128 Z+jets 0\n",
      "mc_364129.Ztautau_PTV0_70_CFilterBVeto.2lep.root\n",
      "364129 Z+jets 0\n",
      "mc_364130.Ztautau_PTV0_70_BFilter.2lep.root\n",
      "364130 Z+jets 0\n",
      "mc_364131.Ztautau_PTV70_140_CVetoBVeto.2lep.root\n",
      "364131 Z+jets 0\n",
      "mc_364132.Ztautau_PTV70_140_CFilterBVeto.2lep.root\n",
      "364132 Z+jets 0\n",
      "mc_364133.Ztautau_PTV70_140_BFilter.2lep.root\n",
      "364133 Z+jets 0\n",
      "mc_364134.Ztautau_PTV140_280_CVetoBVeto.2lep.root\n",
      "364134 Z+jets 0\n",
      "mc_364135.Ztautau_PTV140_280_CFilterBVeto.2lep.root\n",
      "364135 Z+jets 0\n",
      "mc_364136.Ztautau_PTV140_280_BFilter.2lep.root\n",
      "364136 Z+jets 0\n",
      "mc_364137.Ztautau_PTV280_500_CVetoBVeto.2lep.root\n",
      "364137 Z+jets 0\n",
      "mc_364138.Ztautau_PTV280_500_CFilterBVeto.2lep.root\n",
      "364138 Z+jets 0\n",
      "mc_364139.Ztautau_PTV280_500_BFilter.2lep.root\n",
      "364139 Z+jets 0\n",
      "mc_364140.Ztautau_PTV500_1000.2lep.root\n",
      "364140 Z+jets 0\n",
      "mc_364141.Ztautau_PTV1000_E_CMS.2lep.root\n",
      "364141 Z+jets 0\n",
      "mc_364156.Wmunu_PTV0_70_CVetoBVeto.2lep.root\n",
      "364156 W+jets 0\n",
      "mc_364157.Wmunu_PTV0_70_CFilterBVeto.2lep.root\n",
      "364157 W+jets 0\n",
      "mc_364158.Wmunu_PTV0_70_BFilter.2lep.root\n",
      "364158 W+jets 0\n",
      "mc_364159.Wmunu_PTV70_140_CVetoBVeto.2lep.root\n",
      "364159 W+jets 0\n",
      "mc_364160.Wmunu_PTV70_140_CFilterBVeto.2lep.root\n",
      "364160 W+jets 0\n",
      "mc_364161.Wmunu_PTV70_140_BFilter.2lep.root\n",
      "364161 W+jets 0\n",
      "mc_364162.Wmunu_PTV140_280_CVetoBVeto.2lep.root\n",
      "364162 W+jets 0\n",
      "mc_364163.Wmunu_PTV140_280_CFilterBVeto.2lep.root\n",
      "364163 W+jets 0\n",
      "mc_364164.Wmunu_PTV140_280_BFilter.2lep.root\n",
      "364164 W+jets 0\n",
      "mc_364165.Wmunu_PTV280_500_CVetoBVeto.2lep.root\n",
      "364165 W+jets 0\n",
      "mc_364166.Wmunu_PTV280_500_CFilterBVeto.2lep.root\n",
      "364166 W+jets 0\n",
      "mc_364167.Wmunu_PTV280_500_BFilter.2lep.root\n",
      "364167 W+jets 0\n",
      "mc_364168.Wmunu_PTV500_1000.2lep.root\n",
      "364168 W+jets 0\n",
      "mc_364169.Wmunu_PTV1000_E_CMS.2lep.root\n",
      "364169 W+jets 0\n",
      "mc_364184.Wtaunu_PTV0_70_CVetoBVeto.2lep.root\n",
      "364184 W+jets 0\n",
      "mc_364185.Wtaunu_PTV0_70_CFilterBVeto.2lep.root\n",
      "364185 W+jets 0\n",
      "mc_364186.Wtaunu_PTV0_70_BFilter.2lep.root\n",
      "364186 W+jets 0\n",
      "mc_364187.Wtaunu_PTV70_140_CVetoBVeto.2lep.root\n",
      "364187 W+jets 0\n",
      "mc_364188.Wtaunu_PTV70_140_CFilterBVeto.2lep.root\n",
      "364188 W+jets 0\n",
      "mc_364189.Wtaunu_PTV70_140_BFilter.2lep.root\n",
      "364189 W+jets 0\n",
      "mc_364190.Wtaunu_PTV140_280_CVetoBVeto.2lep.root\n",
      "364190 W+jets 0\n",
      "mc_364191.Wtaunu_PTV140_280_CFilterBVeto.2lep.root\n",
      "364191 W+jets 0\n",
      "mc_364192.Wtaunu_PTV140_280_BFilter.2lep.root\n",
      "364192 W+jets 0\n",
      "mc_364193.Wtaunu_PTV280_500_CVetoBVeto.2lep.root\n",
      "364193 W+jets 0\n",
      "mc_364194.Wtaunu_PTV280_500_CFilterBVeto.2lep.root\n",
      "364194 W+jets 0\n",
      "mc_364195.Wtaunu_PTV280_500_BFilter.2lep.root\n",
      "364195 W+jets 0\n",
      "mc_364196.Wtaunu_PTV500_1000.2lep.root\n",
      "364196 W+jets 0\n",
      "mc_364197.Wtaunu_PTV1000_E_CMS.2lep.root\n",
      "364197 W+jets 0\n",
      "mc_410000.ttbar_lep.2lep.root\n",
      "410000 ttbar 0\n",
      "mc_410011.single_top_tchan.2lep.root\n",
      "410011 singleTop 0\n",
      "mc_410013.single_top_wtchan.2lep.root\n",
      "410013 singleTop 0\n",
      "mc_410025.single_top_schan.2lep.root\n",
      "410025 singleTop 0\n"
     ]
    }
   ],
   "source": [
    "infile1.clear();\n",
    "infile1.seekg(0, ios::beg);  // Start at the beginning of the file\n",
    "\n",
    "background->Reset(); // Reset the TChain (if necessary)\n",
    "DSID = 0;\n",
    "\n",
    "//while (!infile.fail() && !infile.eof() ){\n",
    "while (infile1 >> sample >> DSID >> type >> mass){\n",
    "        cout << sample << endl;\n",
    "        path = \"../../../Downloads/2lep/MC/\"+sample;\n",
    "    //        path = \"../Input/13TeV/MC/\"+sample;\n",
    "        background->Add(path);\n",
    "        dataset_IDs.push_back(DSID);\n",
    "        types.push_back(type);\n",
    "        masses.push_back(mass);\n",
    "        cout << DSID << \" \" << type << \" \" << mass << endl;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifstream infile2(\"../Input/13TeV/dataset_dataInput.txt\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data->Reset();\n",
    "infile2.clear();\n",
    "infile2.seekg(0, ios::beg);\n",
    "\n",
    "while (infile2 >> sample){\n",
    "        path = \"../../../Downloads/2lep/Data/\"+sample;\n",
    "//        path = \"../Input/13TeV/Data/\"+sample;\n",
    "        data->Add(path);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Add individual DSIDs to corresponding types\n",
    "vector<Int_t> Zjets, ttbar, graviton, Zprime, Dibosons, tt, Wjets, WZ, singleTop, Higgs;\n",
    "vector<Int_t> signal_masses, background_IDs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "Zjets.clear();\n",
    "ttbar.clear();\n",
    "Dibosons.clear();\n",
    "graviton.clear();\n",
    "Zprime.clear();\n",
    "tt.clear();\n",
    "Wjets.clear();\n",
    "\n",
    "for(int j=0; j<types.size(); j++){\n",
    "    if (types[j] == \"Z+jets\"){\n",
    "        Zjets.push_back(dataset_IDs[j]); \n",
    "        background_IDs.push_back(dataset_IDs[j]);\n",
    "    }\n",
    "    else if (types[j] == \"ttbar\"){\n",
    "        ttbar.push_back(dataset_IDs[j]); \n",
    "        background_IDs.push_back(dataset_IDs[j]);\n",
    "    }\n",
    "    else if (types[j] == \"Graviton\"){\n",
    "        graviton.push_back(dataset_IDs[j]);\n",
    "        signal_masses.push_back(masses[j]);\n",
    "    }\n",
    "    else if (types[j] == \"Zprime\"){\n",
    "        Zprime.push_back(dataset_IDs[j]); \n",
    "        background_IDs.push_back(dataset_IDs[j]);\n",
    "    }\n",
    "    else if (types[j] == \"Dibosons\"){\n",
    "        Dibosons.push_back(dataset_IDs[j]);\n",
    "        background_IDs.push_back(dataset_IDs[j]);\n",
    "    }\n",
    "    else if (types[j] == \"tt\"){\n",
    "        tt.push_back(dataset_IDs[j]); \n",
    "        background_IDs.push_back(dataset_IDs[j]);\n",
    "    }\n",
    "    else if (types[j] == \"W+jets\"){\n",
    "        Wjets.push_back(dataset_IDs[j]); \n",
    "        background_IDs.push_back(dataset_IDs[j]);\n",
    "    }\n",
    "    else if (types[j] == \"Higgs\"){\n",
    "        Higgs.push_back(dataset_IDs[j]); \n",
    "        background_IDs.push_back(dataset_IDs[j]);\n",
    "    }\n",
    "    else if (types[j] == \"WZ\"){\n",
    "        WZ.push_back(dataset_IDs[j]); \n",
    "        background_IDs.push_back(dataset_IDs[j]);\n",
    "    }\n",
    "    else if (types[j] == \"singleTop\"){\n",
    "        singleTop.push_back(dataset_IDs[j]); \n",
    "        background_IDs.push_back(dataset_IDs[j]);\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "for (int i=0; i<signal_masses.size(); i++){ cout << signal_masses[i] << endl; }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Assign branch variables to variables defined in root file\n",
    "\n",
    "Int_t lep_n;\n",
    "Int_t channelNumber;\n",
    "Float_t XSection, met_et, mcWeight, SumWeights;\n",
    "Bool_t trigE, trigM;\n",
    "vector<Int_t> *lep_type, *lep_charge;\n",
    "vector<Float_t> *lep_pt, *lep_E, *lep_phi, *lep_eta, *lep_etcone20, *lep_ptcone30, *lep_z0, *lep_trackd0pvunbiased, *lep_tracksigd0pvunbiased;\n",
    "vector<Bool_t> *trigMatched, *lep_isTightID;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Float_t scaleFactor_PILEUP, scaleFactor_ELE, scaleFactor_MUON, scaleFactor_BTAG, scaleFactor_lepTRIGGER;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Obtain header variables\n",
    "\n",
    "// For MC:  \n",
    "background->SetBranchAddress(\"lep_n\",      &lep_n);\n",
    "background->SetBranchAddress(\"lep_charge\", &lep_charge);\n",
    "background->SetBranchAddress(\"lep_type\",   &lep_type);\n",
    "background->SetBranchAddress(\"lep_pt\",     &lep_pt);\n",
    "background->SetBranchAddress(\"lep_eta\",    &lep_eta);\n",
    "background->SetBranchAddress(\"lep_phi\",    &lep_phi);\n",
    "background->SetBranchAddress(\"lep_E\",      &lep_E);\n",
    "background->SetBranchAddress(\"met_et\",     &met_et); \n",
    "background->SetBranchAddress(\"channelNumber\", &channelNumber);\n",
    "background->SetBranchAddress(\"mcWeight\", &mcWeight);\n",
    "background->SetBranchAddress(\"scaleFactor_PILEUP\", &scaleFactor_PILEUP );\n",
    "background->SetBranchAddress(\"scaleFactor_ELE\", &scaleFactor_ELE ); \n",
    "background->SetBranchAddress(\"scaleFactor_MUON\", &scaleFactor_MUON ); \n",
    "background->SetBranchAddress(\"scaleFactor_BTAG\", &scaleFactor_BTAG );\n",
    "background->SetBranchAddress(\"scaleFactor_LepTRIGGER\", &scaleFactor_lepTRIGGER );\n",
    "background->SetBranchAddress(\"lep_isTightID\", &lep_isTightID);\n",
    "background->SetBranchAddress(\"lep_ptcone30\", &lep_ptcone30);\n",
    "background->SetBranchAddress(\"lep_etcone20\", &lep_etcone20);\n",
    "background->SetBranchAddress(\"trigE\", &trigE);\n",
    "background->SetBranchAddress(\"trigM\", &trigM);\n",
    "background->SetBranchAddress(\"SumWeights\", &SumWeights);\n",
    "background->SetBranchAddress(\"XSection\", &XSection); \n",
    "background->SetBranchAddress(\"lep_z0\", &lep_z0);\n",
    "background->SetBranchAddress(\"lep_trackd0pvunbiased\", &lep_trackd0pvunbiased);\n",
    "background->SetBranchAddress(\"lep_tracksigd0pvunbiased\", &lep_tracksigd0pvunbiased);\n",
    "\n",
    "// For data:\n",
    "data->SetBranchAddress(\"lep_n\",      &lep_n);\n",
    "data->SetBranchAddress(\"lep_charge\", &lep_charge);\n",
    "data->SetBranchAddress(\"lep_type\",   &lep_type);\n",
    "data->SetBranchAddress(\"lep_pt\",     &lep_pt);\n",
    "data->SetBranchAddress(\"lep_eta\",    &lep_eta);\n",
    "data->SetBranchAddress(\"lep_phi\",    &lep_phi);\n",
    "data->SetBranchAddress(\"lep_E\",      &lep_E);\n",
    "data->SetBranchAddress(\"met_et\",     &met_et); \n",
    "data->SetBranchAddress(\"channelNumber\", &channelNumber);\n",
    "data->SetBranchAddress(\"trigE\", &trigE); \n",
    "data->SetBranchAddress(\"trigM\", &trigM);\n",
    "data->SetBranchAddress(\"lep_isTightID\", &lep_isTightID);\n",
    "data->SetBranchAddress(\"lep_ptcone30\", &lep_ptcone30); \n",
    "data->SetBranchAddress(\"lep_etcone20\", &lep_etcone20);\n",
    "data->SetBranchAddress(\"lep_z0\", &lep_z0);\n",
    "data->SetBranchAddress(\"lep_trackd0pvunbiased\", &lep_trackd0pvunbiased);\n",
    "data->SetBranchAddress(\"lep_tracksigd0pvunbiased\", &lep_tracksigd0pvunbiased);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating histograms over all MC DSIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "\n",
    "2. Analysis of data and obtaining the desired final states by cuts\n",
    "\n",
    "*/\n",
    "\n",
    "// 1. Create map of histograms for MC events\n",
    "\n",
    "// Declaring map containing 1D histograms, in which each histogram contains values of type Int_t\n",
    "map<Int_t, TH1*> hist_mll; \n",
    "map<Int_t, TH1*> hist_lep_pt;\n",
    "map<Int_t, TH1*> hist_met;\n",
    "\n",
    "// Assign each DSID in datasetIDs as a unique input element i in histograms\n",
    "for(const auto & i:dataset_IDs){\n",
    "    hist_mll[i] = new TH1F();\n",
    "    hist_lep_pt[i] = new TH1F(); \n",
    "    hist_met[i] = new TH1F();\n",
    "}\n",
    "\n",
    "//for(const auto & i:dataset_IDs){\n",
    "for(int i=0; i<dataset_IDs.size(); i++){\n",
    "    hist_mll[dataset_IDs[i]]->SetNameTitle(Form(\"%d\", dataset_IDs[i]), \"Invariant mass\");\n",
    "    hist_lep_pt[dataset_IDs[i]]->SetNameTitle(Form(\"%d\", dataset_IDs[i]), \"Lepton pT\"); \n",
    "    hist_met[dataset_IDs[i]]->SetNameTitle(Form(\"%d\", dataset_IDs[i]), \"Missing ET\");\n",
    "    if (masses[i] == 750){ hist_mll[dataset_IDs[i]]->SetBins(150, 0, 5000); }\n",
    "    else if (masses[i] == 1000){ hist_mll[dataset_IDs[i]]->SetBins(100, 0, 5000); }\n",
    "    else if (masses[i] == 2000){ hist_mll[dataset_IDs[i]]->SetBins(20, 0, 5000); }\n",
    "    else if (masses[i] == 3000){ hist_mll[dataset_IDs[i]]->SetBins(5, 0, 5000); }\n",
    "    else if (masses[i] == 4000){ hist_mll[dataset_IDs[i]]->SetBins(3, 0, 5000); }\n",
    "    else{ hist_mll[dataset_IDs[i]]->SetBins(70,0,5000); }\n",
    "    hist_lep_pt[dataset_IDs[i]]->SetBins(30,0,2000);\n",
    "    hist_met[dataset_IDs[i]]->SetBins(30,0,3000); \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Map of histograms for MC events for separate channels (mumu, ee)\n",
    "map<Int_t, TH1*> hist_mll_ee; \n",
    "map<Int_t, TH1*> hist_lep_pt_ee;\n",
    "map<Int_t, TH1*> hist_met_ee;\n",
    "\n",
    "map<Int_t, TH1*> hist_mll_uu; \n",
    "map<Int_t, TH1*> hist_lep_pt_uu;\n",
    "map<Int_t, TH1*> hist_met_uu;\n",
    "\n",
    "// Assign each DSID in datasetIDs as a unique input element i in histograms\n",
    "for(const auto & i:dataset_IDs){\n",
    "    hist_mll_ee[i] = new TH1F();\n",
    "    hist_lep_pt_ee[i] = new TH1F(); \n",
    "    hist_met_ee[i] = new TH1F();\n",
    "    \n",
    "    hist_mll_uu[i] = new TH1F();\n",
    "    hist_lep_pt_uu[i] = new TH1F(); \n",
    "    hist_met_uu[i] = new TH1F();\n",
    "}\n",
    "\n",
    "//for(const auto & i:dataset_IDs){\n",
    "for(int i=0; i<dataset_IDs.size(); i++){\n",
    "    hist_mll_ee[dataset_IDs[i]]->SetNameTitle(Form(\"%d\", dataset_IDs[i]), \"Invariant mass\");\n",
    "    hist_lep_pt_ee[dataset_IDs[i]]->SetNameTitle(Form(\"%d\", dataset_IDs[i]), \"Lepton pT\"); \n",
    "    hist_met_ee[dataset_IDs[i]]->SetNameTitle(Form(\"%d\", dataset_IDs[i]), \"Missing ET\");\n",
    "    hist_mll_ee[dataset_IDs[i]]->SetBins(70,0,5000); \n",
    "    hist_lep_pt_ee[dataset_IDs[i]]->SetBins(30,0,2000);\n",
    "    hist_met_ee[dataset_IDs[i]]->SetBins(30,0,3000); \n",
    "    \n",
    "    hist_mll_uu[dataset_IDs[i]]->SetNameTitle(Form(\"%d\", dataset_IDs[i]), \"Invariant mass\");\n",
    "    hist_lep_pt_uu[dataset_IDs[i]]->SetNameTitle(Form(\"%d\", dataset_IDs[i]), \"Lepton pT\"); \n",
    "    hist_met_uu[dataset_IDs[i]]->SetNameTitle(Form(\"%d\", dataset_IDs[i]), \"Missing ET\");\n",
    "    hist_mll_uu[dataset_IDs[i]]->SetBins(70,0,5000); \n",
    "    hist_lep_pt_uu[dataset_IDs[i]]->SetBins(30,0,2000);\n",
    "    hist_met_uu[dataset_IDs[i]]->SetBins(30,0,3000); \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Map of signal (mll) MC histograms only! (The initial amount of events sent inn before cuts are applied)\n",
    "map<Int_t, TH1*> hist_mll_signal_init; \n",
    "\n",
    "for(const auto & i:graviton){\n",
    "    hist_mll_signal_init[i] = new TH1F();\n",
    "}\n",
    "\n",
    "for(int i=0; i<graviton.size(); i++){\n",
    "    hist_mll_signal_init[graviton[i]]->SetNameTitle(Form(\"%d\", graviton[i]), \"Invariant mass\");\n",
    "    if (signal_masses[i] == 750){ hist_mll_signal_init[graviton[i]]->SetBins(150, 0, 5000); }\n",
    "    else if (signal_masses[i] == 1000){ hist_mll_signal_init[graviton[i]]->SetBins(100, 0, 5000); }\n",
    "    else if (signal_masses[i] == 2000){ hist_mll_signal_init[graviton[i]]->SetBins(20, 0, 5000); }\n",
    "    else if (signal_masses[i] == 3000){ hist_mll_signal_init[graviton[i]]->SetBins(5, 0, 5000); }\n",
    "    else if (signal_masses[i] == 4000){ hist_mll_signal_init[graviton[i]]->SetBins(3, 0, 5000); }\n",
    "    else{ hist_mll_signal_init[graviton[i]]->SetBins(70,0,5000); }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating histograms for recorded data events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "// 2. Create histogram for recorded data events\n",
    "\n",
    "hist_mll_d = new TH1F(); \n",
    "hist_lep_pt_d = new TH1F(); \n",
    "hist_met_d = new TH1F();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_mll_d->SetNameTitle(\"hist_mll\", \"Invariant mass\");\n",
    "hist_lep_pt_d->SetNameTitle(\"hist_lep_pt\", \"Lepton pT\"); \n",
    "hist_met_d->SetNameTitle(\"hist_met\", \"Missing ET\");\n",
    "hist_mll_d->SetBins(70,0,5000);\n",
    "hist_lep_pt_d->SetBins(30,0,2000);\n",
    "hist_met_d->SetBins(30,0,3000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create histogram for recorded data events for separate channels (ee, mumu)\n",
    "hist_mll_d_ee = new TH1F(); \n",
    "hist_lep_pt_d_ee = new TH1F(); \n",
    "hist_met_d_ee = new TH1F();\n",
    "\n",
    "hist_mll_d_uu = new TH1F(); \n",
    "hist_lep_pt_d_uu = new TH1F(); \n",
    "hist_met_d_uu = new TH1F();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_mll_d_ee->SetNameTitle(\"hist_mll\", \"Invariant mass\");\n",
    "hist_lep_pt_d_ee->SetNameTitle(\"hist_lep_pt\", \"Lepton pT\"); \n",
    "hist_met_d_ee->SetNameTitle(\"hist_met\", \"Missing ET\");\n",
    "hist_mll_d_ee->SetBins(70,0,5000);\n",
    "hist_lep_pt_d_ee->SetBins(30,0,2000);\n",
    "hist_met_d_ee->SetBins(30,0,3000);\n",
    "\n",
    "hist_mll_d_uu->SetNameTitle(\"hist_mll\", \"Invariant mass\");\n",
    "hist_lep_pt_d_uu->SetNameTitle(\"hist_lep_pt\", \"Lepton pT\"); \n",
    "hist_met_d_uu->SetNameTitle(\"hist_met\", \"Missing ET\");\n",
    "hist_mll_d_uu->SetBins(70,0,5000);\n",
    "hist_lep_pt_d_uu->SetBins(30,0,2000);\n",
    "hist_met_d_uu->SetBins(30,0,3000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating histograms for signal + background events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "// 3. Create histogram for signal + background events\n",
    "hist_mll_sb = new TH1F();\n",
    "hist_lep_pt_sb = new TH1F();\n",
    "hist_met_sb = new TH1F();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_mll_sb->SetNameTitle(\"\", \"Invariant mass\"); \n",
    "hist_lep_pt_sb->SetNameTitle(\"hist_lep_pt\", \"Lepton pT\"); \n",
    "hist_met_sb->SetNameTitle(\"hist_met\", \"Missing ET\");\n",
    "hist_mll_sb->SetBins(70,0,5000); \n",
    "hist_lep_pt_sb->SetBins(30,0,2000);\n",
    "hist_met_sb->SetBins(30,0,3000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Lorentz vectors for decay products\n",
    "TLorentzVector l1, l2, dileptons, l1temp, l2temp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TChain *dataset = new TChain(\"mini\");\n",
    "TString channel;\n",
    "vector<Int_t> DSIDs;// datasetIDs\n",
    "int isData; \n",
    "int nentries; //number of entries in each input file\n",
    "int zjets, Ttbar, dibosons, gravitons, zprime, j, Tt, wjets, higgs, singletop, wz;\n",
    "int Gee, Guu, Wmunu, Wenu, G_init_uu, b; // counters\n",
    "Float_t W; // weight\n",
    "Double_t L;   //Integrated luminosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10000.; //(pb)^-1\n",
    "DSIDs.push_back(0);\n",
    "j = zjets = Ttbar = gravitons = zprime = dibosons = Tt = wjets = higgs = singletop = wz = 0;\n",
    "Gee = Guu = Wmunu = Wenu = G_init_uu = b = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Reset histograms (in case you have filled them before) \n",
    "for(const auto & i:dataset_IDs){ \n",
    "    hist_mll[i]->Reset(); \n",
    "    hist_lep_pt[i]->Reset(); \n",
    "    hist_met[i]->Reset();\n",
    "    \n",
    "    hist_mll_ee[i]->Reset();\n",
    "    hist_lep_pt_ee[i]->Reset(); \n",
    "    hist_met_ee[i]->Reset();\n",
    "\n",
    "    hist_mll_uu[i]->Reset();\n",
    "    hist_lep_pt_uu[i]->Reset(); \n",
    "    hist_met_uu[i]->Reset();\n",
    "}\n",
    "\n",
    "hist_mll_d->Reset();\n",
    "hist_lep_pt_d->Reset(); \n",
    "hist_met_d->Reset(); \n",
    "\n",
    "hist_mll_d_ee->Reset();\n",
    "hist_lep_pt_d_ee->Reset(); \n",
    "hist_met_d_ee->Reset(); \n",
    "\n",
    "hist_mll_d_uu->Reset();\n",
    "hist_lep_pt_d_uu->Reset(); \n",
    "hist_met_d_uu->Reset(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuts for the individual lepton pairs:\n",
    "The primary vertex chosen is the one with the highest summed $p_T^2$ of tracks with transverse momentum $p_T > 0.5$GeV associated with the vertex.\n",
    "\n",
    "1. __Preselection and final cut requirements__:\n",
    "    - Require only 2 leptons in the final state\n",
    "    - Require same-flavour leptons\n",
    "    - Muon pair must have opposite charges\n",
    "    - Electron pair has no requirement on opposite charges due to the higher probability of charge misidentification for high $E_T$-electrons (if we had an opposite charge requirement).\n",
    "    \n",
    "\n",
    "2. __Reconstructed electrons must__\n",
    "   - have $E_T > 30$GeV\n",
    "   - have $|\\eta| < 2.47$ (in order to pass through the fine-granularity region of the EM calorimeter)\n",
    "   - be outside the range $1.37 < |\\eta| < 1.52$ (corresponding to the transition region between the barrel and endcap EM calorimeters)\n",
    "   - have $|z_0\\sin\\Theta| < 0.5$mm (in order to be consistent with the primary vertex along the beamline)\n",
    "   - have $|d_0/\\sigma(d_0)| < 5$ (in order to be consistent with the primary vertex transverse to the beamline)\n",
    "   \n",
    "\n",
    "3. __Reconstructed muons must__\n",
    "    - have $p_T > 30$GeV\n",
    "    - have $|\\eta| < 2.5$ (in order to pass through the fine-granularity region of the EM calorimeter)\n",
    "    - be outside the range $1.01 < |\\eta| < 1.1$ (corresponding to the transition region between the barrel and endcap EM calorimeters)\n",
    "    - have $|z_0\\sin\\Theta| < 0.5$mm (in order to be consistent with the primary vertex along the beamline)\n",
    "    - have $|d_0/\\sigma(d_0)| < 3$ (in order to be consistent with the primary vertex transverse to the beamline)\n",
    "    - have that the summed scalar $p_T$ of good-quality tracks with $p_T > 1GeV$ originating from the primary vertex within a cone of variable size $\\Delta R$ (max R=0.3) around the muon, but excluding the muon-candidate track itself, must be less than 6% of the $p_T$ of the muon candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running over background...\n",
      "Number entries: 42885659\n",
      "------------------------------------------\n",
      "1 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "2 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "3 million events processed\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=0 fEntryCurrent=0 fNextClusterStart=55035 but fEntryCurrent should not be in between the two\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "4 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "5 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "6 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "7 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "8 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "9 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "10 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "11 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "12 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "13 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "14 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "15 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "16 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "17 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "18 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "19 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "20 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "21 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "22 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "23 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "24 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "25 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "26 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "27 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "28 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "29 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "30 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "31 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "32 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "33 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "34 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "35 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "36 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "37 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "38 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "39 million events processed\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in <TTreeCache::FillBuffer>: Inconsistency: fCurrentClusterStart=0 fEntryCurrent=0 fNextClusterStart=18447 but fEntryCurrent should not be in between the two\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "40 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "41 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "42 million events processed\n",
      "------------------------------------------\n",
      "Running over data...\n",
      "Number entries: 12205790\n",
      "------------------------------------------\n",
      "1 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "2 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "3 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "4 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "5 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "6 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "7 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "8 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "9 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "10 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "11 million events processed\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "12 million events processed\n",
      "------------------------------------------\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "// Loop through all events in background and data separately\n",
    "for(isData = 0; isData<2; isData++){\n",
    "    if(isData == 1){\n",
    "        nentries = data->GetEntries();\n",
    "        dataset = data;\n",
    "        cout << \"Running over data...\" << endl; \n",
    "        cout << \"Number entries: \" << nentries << endl;\n",
    "    }\n",
    "    else {\n",
    "        nentries = background->GetEntries();\n",
    "        dataset = background;\n",
    "        cout << \"Running over background...\" << endl;\n",
    "        cout << \"Number entries: \" << nentries << endl;\n",
    "    }\n",
    "\n",
    "    //running over all events within data and background\n",
    "    for (int i = 0; i < nentries; i++){    //i<nentries \n",
    "        \n",
    "        if(i%1000000 == 0 && i>0){ cout << \"------------------------------------------\" << endl;}\n",
    "        if(i%1000000 == 0 && i>0){ cout << i/1000000 << \" million events processed\" << endl;}\n",
    "        if(i%1000000 == 0 && i>0){ cout << \"------------------------------------------\" << endl;}\n",
    "\n",
    "         // We \"pull out\" the i'th entry in the chain. The variables are now \n",
    "         // available through the names we have given them.\n",
    "        dataset->GetEntry(i);\n",
    "           \n",
    "        \n",
    "        // Preselection (variables are stored in the TTree with unit MeV, so we need to divide by 1000 \n",
    "        // to get GeV, which is a more practical and commonly used unit):\n",
    "\n",
    "        // Fill initial amount of MC signal events\n",
    "        if(std::find(graviton.begin(), graviton.end(), channelNumber) != graviton.end()){\n",
    "            if (fabs(lep_type->at(0)) == 13 && fabs(lep_type->at(0)) == fabs(lep_type->at(1))){\n",
    "                // Set Lorentz vectors\n",
    "                l1.SetPtEtaPhiE(lep_pt->at(0)/1000., lep_eta->at(0), lep_phi->at(0), lep_E->at(0)/1000.);\n",
    "                l2.SetPtEtaPhiE(lep_pt->at(1)/1000., lep_eta->at(1), lep_phi->at(1), lep_E->at(1)/1000.);\n",
    "                // Set invariant mass\n",
    "                dileptons = l1 + l2;\n",
    "                // Scaling\n",
    "                W = (XSection*L/SumWeights)*mcWeight*scaleFactor_PILEUP*scaleFactor_ELE*scaleFactor_MUON*scaleFactor_BTAG*scaleFactor_lepTRIGGER; // *scaleFactor_JVFSF*scaleFactor_ZVERTEX;             \n",
    "\n",
    "                hist_mll_signal_init[channelNumber]->Fill(dileptons.M(), W);\n",
    "                G_init_uu++;\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if (fabs(lep_type->at(0))==13 && fabs(lep_type->at(0)) == fabs(lep_type->at(1))){ b++; }\n",
    "        \n",
    "        // Cut #1: Require (exactly) 2 leptons\n",
    "        if(lep_n != 2){ continue; }\n",
    "        // Cut #2: Require same flavour (2 electrons or 2 muons)\n",
    "        if(fabs(lep_type->at(0)) != fabs(lep_type->at(1))){ continue; }\n",
    "\n",
    "        // Set Lorentz vectors\n",
    "        l1.SetPtEtaPhiE(lep_pt->at(0)/1000., lep_eta->at(0), lep_phi->at(0), lep_E->at(0)/1000.);\n",
    "        l2.SetPtEtaPhiE(lep_pt->at(1)/1000., lep_eta->at(1), lep_phi->at(1), lep_E->at(1)/1000.);\n",
    "        \n",
    "        // Cut #3: Cut on z_0 impact parameter\n",
    "        if( fabs(lep_z0->at(0)*TMath::Sin(l1.Theta())) > 0.5 ){ continue; }\n",
    "        if( fabs(lep_z0->at(1)*TMath::Sin(l2.Theta())) > 0.5 ){ continue; }\n",
    "\n",
    "        // Cut #4: Lepton isolation cuts\n",
    "//        if( lep_ptcone30->at(0)/(lep_pt->at(0)) > 0.15 ){ continue; }\n",
    "//        if( lep_ptcone30->at(1)/(lep_pt->at(1)) > 0.15 ){ continue; }\n",
    "\n",
    "//        if( lep_etcone20->at(0)/(lep_pt->at(0)) > 0.15 ){ continue; }\n",
    "//        if( lep_etcone20->at(1)/(lep_pt->at(1)) > 0.15 ){ continue; }\n",
    "\n",
    "        if( lep_etcone20->at(0)/(lep_pt->at(0)) < -0.1 || lep_etcone20->at(0)/(lep_pt->at(0)) > 0.2 ){ continue; } // || = or operator\n",
    "        if( lep_etcone20->at(1)/(lep_pt->at(1)) < -0.1 || lep_etcone20->at(1)/(lep_pt->at(1)) > 0.2 ){ continue; } // || = or operator\n",
    "\n",
    "        if( lep_pt->at(0)/1000.0 < 30 ){ continue; }        \n",
    "        if( lep_pt->at(1)/1000.0 < 30 ){ continue; }       \n",
    "       \n",
    "        // Identify leptons\n",
    "        if( fabs(lep_type->at(0)) == 11 ){ channel = \"ee\"; }\n",
    "        if( fabs(lep_type->at(0)) == 13 ){ channel = \"uu\"; }\n",
    "        \n",
    "        // Cuts on individual muons and electrons:       \n",
    "        // Electrons\n",
    "        if( channel == \"ee\"){\n",
    "            if( fabs(lep_eta->at(0)) > 2.47 ){ continue; }\n",
    "            if( fabs(lep_eta->at(0)) > 1.37 && fabs(lep_eta->at(0)) < 1.52){ continue; }\n",
    "            if( lep_E->at(0)/1000.0 < 30 ){ continue; }\n",
    "            if( fabs(lep_trackd0pvunbiased->at(0)/(lep_tracksigd0pvunbiased->at(0))) > 5 ){ continue; }\n",
    "\n",
    "            if( lep_E->at(1)/1000.0 < 30 ){ continue; }\n",
    "            if( fabs(lep_eta->at(1)) > 2.47 ){ continue; }\n",
    "            if( fabs(lep_eta->at(1)) > 1.37 && fabs(lep_eta->at(1)) < 1.52){ continue; }\n",
    "            if( fabs(lep_trackd0pvunbiased->at(1)/(lep_tracksigd0pvunbiased->at(1))) > 5 ){ continue; }\n",
    "            \n",
    "            if( trigE != 1 ){ continue; }\n",
    "        }\n",
    "        \n",
    "        // Muons\n",
    "        if( channel == \"uu\"){\n",
    "            // Require opposite charge\n",
    "            if(lep_charge->at(0) == lep_charge->at(1)){ continue; }\n",
    "\n",
    "            if( lep_ptcone30->at(0) > 0.06*lep_pt->at(0)){ continue; }\n",
    "            if( fabs(lep_eta->at(0)) > 2.5 ){ continue; }\n",
    "            if( fabs(lep_eta->at(0)) > 1.01 && fabs(lep_eta->at(0)) < 1.1){ continue; }\n",
    "            if( fabs(lep_trackd0pvunbiased->at(0)/(lep_tracksigd0pvunbiased->at(0))) > 3 ){ continue; }\n",
    "\n",
    "            if( lep_ptcone30->at(1) > 0.06*lep_pt->at(1)){ continue; }\n",
    "            if( fabs(lep_eta->at(1)) > 2.5 ){ continue; }\n",
    "            if( fabs(lep_eta->at(1)) > 1.01 && fabs(lep_eta->at(1)) < 1.1){ continue; }\n",
    "            if( fabs(lep_trackd0pvunbiased->at(1)/(lep_tracksigd0pvunbiased->at(1))) > 3 ){ continue; }\n",
    "\n",
    "            if( trigM != 1 ){ continue; }\n",
    "        }\n",
    "\n",
    "        // If an event passes all criterias above then obtain invariant mass, pT and missing transverse E\n",
    "               \n",
    "        dileptons = l1 + l2;\n",
    "        if (dileptons.M() < 225){ continue; }    // to avoid the Z-peak region\n",
    "        \n",
    "        // Fill histograms\n",
    "        if(isData == 1)\n",
    "        {\n",
    "            // signal samples only includes G->uu, thus only looking at muon in final state:\n",
    "            if (channel == \"uu\"){\n",
    "                hist_mll_d->Fill(dileptons.M());\n",
    "                hist_lep_pt_d->Fill(l1.Pt());\n",
    "                hist_lep_pt_d->Fill(l2.Pt()); \n",
    "                hist_met_d->Fill(met_et/1000.); \n",
    "            }\n",
    "        }\n",
    "                \n",
    "        else\n",
    "        {\n",
    "            j++;\n",
    "            DSIDs.push_back(channelNumber);\n",
    "            \n",
    "            // Count number of events included in plotting within each background and signal\n",
    "            if(std::find(Zjets.begin(), Zjets.end(), channelNumber) != Zjets.end()){zjets++; }\n",
    "            if(std::find(ttbar.begin(), ttbar.end(), channelNumber) != ttbar.end()){Ttbar++; }\n",
    "            if(std::find(Zprime.begin(), Zprime.end(), channelNumber) != Zprime.end()){zprime++; }\n",
    "            if(std::find(Dibosons.begin(), Dibosons.end(), channelNumber) != Dibosons.end()){dibosons++; }\n",
    "            if(std::find(Wjets.begin(), Wjets.end(), channelNumber) != Wjets.end()){\n",
    "                wjets++;\n",
    "                if (channel==\"ee\"){Wenu++; }\n",
    "                else if(channel==\"uu\"){Wmunu++; }\n",
    "            }\n",
    "            if(std::find(graviton.begin(), graviton.end(), channelNumber) != graviton.end()){ \n",
    "                gravitons++;\n",
    "                if (channel==\"ee\"){Gee++;}\n",
    "                else if(channel==\"uu\"){Guu++; }\n",
    "            }\n",
    "            if(std::find(Higgs.begin(), Higgs.end(), channelNumber) != Higgs.end()){higgs++; }\n",
    "            if(std::find(WZ.begin(), WZ.end(), channelNumber) != WZ.end()){wz++; }\n",
    "            if(std::find(singleTop.begin(), singleTop.end(), channelNumber) != singleTop.end()){singletop++; }\n",
    "\n",
    "            // correction to SumWeights and XSection for G->mumu (4000GeV)\n",
    "            if(channelNumber==305580){\n",
    "                SumWeights = 48000;\n",
    "                XSection = 6.41*pow(10, -5);\n",
    "            }\n",
    "            \n",
    "            // Scaling\n",
    "            W = (XSection*L/SumWeights)*mcWeight*scaleFactor_PILEUP*scaleFactor_ELE*scaleFactor_MUON*scaleFactor_BTAG*scaleFactor_lepTRIGGER; // *scaleFactor_JVFSF*scaleFactor_ZVERTEX;             \n",
    "\n",
    "            if (channel == \"uu\"){\n",
    "                hist_mll[channelNumber]->Fill(dileptons.M(), W);\n",
    "                hist_lep_pt[channelNumber]->Fill(l1.Pt(), W);\n",
    "                hist_lep_pt[channelNumber]->Fill(l2.Pt(), W);\n",
    "                hist_met[channelNumber]->Fill(met_et/1000., W);            \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "cout << \"Done!\" << endl;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zjets: 62187, ttbar: 69884, Zprime: 0, dibosons: 156052, tt: 0, wjets: 301, Higgs: 2133, WZ: 2339, singleTop: 4991\n",
      "Initial number of gravitons: 127224\n",
      "Initial number of background events: 32612891\n",
      "gravitons: 116594 (Gee = 9, Guu = 116585)\n",
      "wjets: 301 (Wenu = 228, Wmunu = 73)\n",
      "DSIDs size: 414482\n",
      "hist_mll: 21104\n",
      "hist_mll_ee: 0\n",
      "hist_mll_uu: 0\n"
     ]
    }
   ],
   "source": [
    "cout << \"zjets: \" << zjets << \", ttbar: \" << Ttbar << \", Zprime: \" << zprime << \", dibosons: \" << dibosons << \", tt: \" << Tt << \", wjets: \" << wjets << \", Higgs: \" << higgs << \", WZ: \" << wz << \", singleTop: \" << singletop <<endl;\n",
    "cout << \"Initial number of gravitons: \" << G_init_uu << endl;\n",
    "cout << \"Initial number of background events: \" << b << endl;\n",
    "cout <<  \"gravitons: \" << gravitons << \" (Gee = \" << Gee << \", Guu = \" << Guu << \")\" << endl;\n",
    "cout << \"wjets: \" << wjets << \" (Wenu = \" << Wenu << \", Wmunu = \" << Wmunu << \")\" << endl;\n",
    "cout << \"DSIDs size: \" << DSIDs.size() <<  endl;\n",
    "cout << \"hist_mll: \" << hist_mll[305580]->GetEntries() << endl;\n",
    "cout << \"hist_mll_ee: \" << hist_mll_ee[305580]->GetEntries() << endl;\n",
    "cout << \"hist_mll_uu: \" << hist_mll_uu[305580]->GetEntries() << endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save histograms to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*******************/\n",
    "/*       MC        */\n",
    "/*******************/\n",
    "TFile ofile_mll_MC(\"hist_mll_MC.root\",\"recreate\");\n",
    "for (const auto i:dataset_IDs) {\n",
    "    hist_mll[i]->Write(Form(\"%d\", i));\n",
    "}\n",
    "ofile_mll_MC.Close();\n",
    "\n",
    "TFile ofile_lep_pt_MC(\"hist_lep_pt_MC.root\", \"recreate\");\n",
    "for (const auto i:dataset_IDs) {\n",
    "    hist_lep_pt[i]->Write(Form(\"%d\", i));\n",
    "}\n",
    "ofile_lep_pt_MC.Close();\n",
    "\n",
    "TFile ofile_met_MC(\"hist_met_MC.root\", \"recreate\");\n",
    "for (const auto i:dataset_IDs) {\n",
    "    hist_met[i]->Write(Form(\"%d\", i));\n",
    "}\n",
    "ofile_met_MC.Close();\n",
    "\n",
    "TFile ofile_mll_signal_init_MC(\"hist_mll_signal_init.root\", \"recreate\");\n",
    "for (const auto i:graviton){\n",
    "    hist_mll_signal_init[i]->Write(Form(\"%d\", i));\n",
    "}\n",
    "\n",
    "/*******************/\n",
    "/*      Data       */\n",
    "/*******************/\n",
    "\n",
    "TFile ofile_mll_dat(\"hist_mll_data.root\",\"recreate\");\n",
    "hist_mll_d->Write();\n",
    "ofile_mll_dat.Close();\n",
    "\n",
    "TFile ofile_lep_pt_dat(\"hist_lep_pt_data.root\", \"recreate\");\n",
    "hist_lep_pt_d->Write();\n",
    "ofile_lep_pt_dat.Close();\n",
    "\n",
    "TFile ofile_met_dat(\"hist_met_data.root\", \"recreate\");\n",
    "hist_met_d->Write();\n",
    "ofile_met_dat.Close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio of data over predicted background events\n",
    "Creating histogram containing ratio of events between data and that predicted by MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "Create histogram containing ratio of events between data and that predicted by MC\n",
    "*/\n",
    "hist_mll_r = new TH1F();\n",
    "hist_pt_r = new TH1F();\n",
    "hist_met_r = new TH1F();\n",
    "\n",
    "hist_mll_r->Reset();\n",
    "hist_pt_r->Reset();\n",
    "hist_met_r->Reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Sum of background histograms\n",
    "hist_mll_b = new TH1F();\n",
    "hist_lep_pt_b = new TH1F();\n",
    "hist_met_b = new TH1F();\n",
    "\n",
    "hist_mll_b->Reset();\n",
    "hist_lep_pt_b->Reset();\n",
    "hist_met_b->Reset();\n",
    "\n",
    "Int_t counter = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(const auto i:dataset_IDs){\n",
    "    cout << \"counter = \" << counter << endl;\n",
    "    if (std::find(graviton.begin(), graviton.end(), i) != graviton.end()){cout << \"DatasetID = \" << i << \" not included in H0.\" << endl; }\n",
    "    else{\n",
    "        cout << \"datasetID = \" << i << \" included in H0.\" << endl;\n",
    "        if( counter== 0){\n",
    "            hist_mll_b = (TH1F*)hist_mll[i]->Clone();\n",
    "            hist_lep_pt_b = (TH1F*)hist_lep_pt[i]->Clone();\n",
    "            hist_met_b = (TH1F*)hist_met[i]->Clone();\n",
    "        }\n",
    "        else{\n",
    "            hist_mll_b->Add(hist_mll[i]);\n",
    "            hist_lep_pt_b->Add(hist_lep_pt[i]);\n",
    "            hist_met_b->Add(hist_met[i]);\n",
    "        }\n",
    "    counter++;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_mll_r = (TH1F*)hist_mll_d->Clone();\n",
    "hist_pt_r = (TH1F*)hist_lep_pt_d->Clone();\n",
    "hist_met_r = (TH1F*)hist_met_d->Clone();\n",
    "\n",
    "hist_mll_r->Divide(hist_mll_d, hist_mll_b);\n",
    "hist_pt_r->Divide(hist_lep_pt_d, hist_lep_pt_b);\n",
    "hist_met_r->Divide(hist_met_d, hist_met_b);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background event histograms\n",
    "Assigning backgrounds according to their dataset ID's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "\n",
    "3. Assign DSID histograms to corresponding background event histograms.\n",
    "\n",
    "Make a new set of histograms, each corresponding to the different background categories, instead of the unique\n",
    "dataset IDs. These histograms are made with the same range and binnings as above.\n",
    "*/\n",
    "\n",
    "map<TString, TH1*> H_mll; \n",
    "map<TString, TH1*> H_lep_pt;\n",
    "map<TString, TH1*> H_met;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector<TString> Backgrounds;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the order of backgrounds according to how many events have been generated for each, in an ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Must define the proper backgrounds for this dataset\n",
    "Backgrounds = {\"Higgs\", \"WZ\", \"singleTop\", \"W+jets\", \"ttbar\", \"Dibosons\", \"Z+jets\"};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(const auto & i:Backgrounds){\n",
    "    cout << \"Background: \" << i << endl;\n",
    "    H_mll[i] = new TH1F();\n",
    "    H_lep_pt[i] = new TH1F(); \n",
    "    H_met[i] = new TH1F();\n",
    "}\n",
    "\n",
    "// Reset histograms (in case you have filled them before) \n",
    "for(const auto i:Backgrounds){\n",
    "    H_mll[i]->Reset();\n",
    "    H_lep_pt[i]->Reset();\n",
    "    H_met[i]->Reset();\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Define properties of background histograms\n",
    "for(const auto & i:Backgrounds){\n",
    "    H_mll[i]->SetNameTitle(\"hist_mll\", \"Invariant mass\");\n",
    "    H_lep_pt[i]->SetNameTitle(\"hist_lep_pt\", \"Lepton pT\");\n",
    "    H_met[i]->SetNameTitle(\"hist_met\", \"Missing ET\");\n",
    "    H_mll[i]->SetBins(70,0,5000); \n",
    "    H_lep_pt[i]->SetBins(30,0,2000);\n",
    "    H_met[i]->SetBins(30,0,3000);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(const auto i:Backgrounds){\n",
    "    H_mll[i]->Reset();\n",
    "    H_lep_pt[i]->Reset();\n",
    "    H_met[i]->Reset();\n",
    "}\n",
    "\n",
    "// add corresponding histograms to respective background processes\n",
    "for(const auto i:dataset_IDs){\n",
    "    if (std::find(ttbar.begin(), ttbar.end(), i) != ttbar.end()){\n",
    "        H_mll[\"ttbar\"]->Add(hist_mll[i]); \n",
    "        H_lep_pt[\"ttbar\"]->Add(hist_lep_pt[i]);\n",
    "        H_met[\"ttbar\"]->Add(hist_met[i]);\n",
    "    }\n",
    "\n",
    "    else if(std::find(Dibosons.begin(), Dibosons.end(), i) != Dibosons.end()){\n",
    "        H_mll[\"Dibosons\"]->Add(hist_mll[i]); \n",
    "        H_lep_pt[\"Dibosons\"]->Add(hist_lep_pt[i]);\n",
    "        H_met[\"Dibosons\"]->Add(hist_met[i]);\n",
    "        cout << \"Found \" << i << \" in Dibosons\" << \"\\n\"; \n",
    "    }\n",
    "\n",
    "    else if(std::find(Wjets.begin(), Wjets.end(), i) != Wjets.end()){\n",
    "        H_mll[\"W+jets\"]->Add(hist_mll[i]); \n",
    "        H_lep_pt[\"W+jets\"]->Add(hist_lep_pt[i]);\n",
    "        H_met[\"W+jets\"]->Add(hist_met[i]);\n",
    "        cout << \"Found \" << i << \" in Wjets\" << \"\\n\"; \n",
    "    }\n",
    "\n",
    "    else if(std::find(Zjets.begin(), Zjets.end(), i) != Zjets.end()){\n",
    "        H_mll[\"Z+jets\"]->Add(hist_mll[i]);\n",
    "        H_lep_pt[\"Z+jets\"]->Add(hist_lep_pt[i]);\n",
    "        H_met[\"Z+jets\"]->Add(hist_met[i]);\n",
    "        cout << \"Found \" << i << \" in Zjets\" << \"\\n\"; \n",
    "    }\n",
    "\n",
    "    else if(std::find(Higgs.begin(), Higgs.end(), i) != Higgs.end()){\n",
    "        H_mll[\"Higgs\"]->Add(hist_mll[i]);\n",
    "        H_lep_pt[\"Higgs\"]->Add(hist_lep_pt[i]);\n",
    "        H_met[\"Higgs\"]->Add(hist_met[i]);\n",
    "        cout << \"Found \" << i << \" in Higgs\" << \"\\n\"; \n",
    "    }\n",
    "\n",
    "    else if(std::find(WZ.begin(), WZ.end(), i) != WZ.end()){\n",
    "        H_mll[\"WZ\"]->Add(hist_mll[i]);\n",
    "        H_lep_pt[\"WZ\"]->Add(hist_lep_pt[i]);\n",
    "        H_met[\"WZ\"]->Add(hist_met[i]);\n",
    "        cout << \"Found \" << i << \" in WZ\" << \"\\n\"; \n",
    "    }\n",
    "\n",
    "    else if(std::find(singleTop.begin(), singleTop.end(), i) != singleTop.end()){\n",
    "        H_mll[\"singleTop\"]->Add(hist_mll[i]);\n",
    "        H_lep_pt[\"singleTop\"]->Add(hist_lep_pt[i]);\n",
    "        H_met[\"singleTop\"]->Add(hist_met[i]);\n",
    "        cout << \"Found \" << i << \" in singleTop\" << \"\\n\"; \n",
    "    }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Making new map containing the colours wanted for each background process, \n",
    "// and set the colours of the histogram\n",
    "\n",
    "map<TString, Int_t> colors;\n",
    "\n",
    "colors[\"Higgs\"] = 920;  //kGray\n",
    "colors[\"WZ\"] = 432-7; //kCyan\n",
    "colors[\"singleTop\"] = 600-7; //kBlue\n",
    "colors[\"W+jets\"] = 616-7; //Magenta\n",
    "colors[\"ttbar\"] = 632-7; //kRed;\n",
    "colors[\"Dibosons\"] = 400-7; //kYellow;\n",
    "colors[\"Z+jets\"] = 416-7; //kGreen;\n",
    "\n",
    "for(const auto h:Backgrounds){\n",
    "    H_mll[h]->SetFillColor(colors[h]); \n",
    "    H_met[h]->SetFillColor(colors[h]);\n",
    "    H_lep_pt[h]->SetFillColor(colors[h]);\n",
    "    \n",
    "    H_mll[h]->SetLineColor(colors[h]); \n",
    "    H_met[h]->SetLineColor(colors[h]);\n",
    "    H_lep_pt[h]->SetLineColor(colors[h]);\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "\n",
    "4. Stack the background histograms\n",
    "\n",
    "For each variable we need to stack the backgrounds on top of each other, which is done by using the THStack class. \n",
    "In the example below we do this for two variables; invariant mass and missing ET.\n",
    "\n",
    "*/\n",
    "\n",
    "THStack *stack_mll = new THStack(\"Invariant mass\", \"\");\n",
    "THStack *stack_met = new THStack(\"Missing ET\", \"\"); \n",
    "THStack *stack_lep_pt = new THStack(\"Lepton pT\", \"\");\n",
    "\n",
    "for(const auto h:Backgrounds){\n",
    "    // Remove previously stacked histograms  \n",
    "    stack_mll->RecursiveRemove(H_mll[h]);\n",
    "    stack_met->RecursiveRemove(H_met[h]);\n",
    "    stack_lep_pt->RecursiveRemove(H_lep_pt[h]);\n",
    "\n",
    "    //Add new stacked histograms\n",
    "    stack_mll->Add(H_mll[h]);\n",
    "    stack_met->Add(H_met[h]);\n",
    "    stack_lep_pt->Add(H_lep_pt[h]);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal event histograms\n",
    "Assigning signal samples according to their dataset ID's and masses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map<TString, TH1*> H_mll_signal; \n",
    "map<TString, TH1*> H_lep_pt_signal;\n",
    "map<TString, TH1*> H_met_signal;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the possible signal samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector<TString> Signals;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// defining the possible signal samples\n",
    "Signals = {\"G (750GeV)\", \"G (1000GeV)\", \"G (2000GeV)\", \"G (3000GeV)\", \"G (4000GeV)\"};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(const auto & i:Signals){\n",
    "    cout << \"Signal: \" << i << endl;\n",
    "    H_mll_signal[i] = new TH1F();\n",
    "    H_lep_pt_signal[i] = new TH1F(); \n",
    "    H_met_signal[i] = new TH1F();\n",
    "}\n",
    "\n",
    "Int_t m;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Define properties of background histograms\n",
    "for(const auto & i:Signals){\n",
    "    H_mll_signal[i]->SetNameTitle(\"hist_mll_signal\", \"Invariant mass\");\n",
    "    H_lep_pt_signal[i]->SetNameTitle(\"hist_lep_pt_signal\", \"Lepton pT\");\n",
    "    H_met_signal[i]->SetNameTitle(\"hist_met_signal\", \"Missing ET\");\n",
    "    if (i==\"G (750GeV)\"){ H_mll_signal[i]->SetBins(150,0,5000); }\n",
    "    else if (i==\"G (1000GeV)\"){ H_mll_signal[i]->SetBins(100,0,5000); }\n",
    "    else if (i==\"G (2000GeV)\"){ H_mll_signal[i]->SetBins(20,0,5000); }\n",
    "    else if (i==\"G (3000GeV)\"){ H_mll_signal[i]->SetBins(5,0,5000); }\n",
    "    else if (i==\"G (4000GeV)\"){ H_mll_signal[i]->SetBins(3,0,5000); }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Reset histograms (in case you have filled them before) \n",
    "for(const auto i:Signals){\n",
    "    H_mll_signal[i]->Reset();\n",
    "    H_lep_pt_signal[i]->Reset();\n",
    "    H_met_signal[i]->Reset();\n",
    "}\n",
    "\n",
    "for (int i=0; i<graviton.size(); i++){\n",
    "    // assign datasetID to each signal sample, corresponding to distinct graviton masses\n",
    "    if(signal_masses[i] == 750){ H_mll_signal[\"G (750GeV)\"]->Add(hist_mll[graviton[i]]); \n",
    "                               cout << \"Added DSID \" << graviton[i] << \" to \" << signal_masses[i] << endl; }\n",
    "    if(signal_masses[i] == 1000){ H_mll_signal[\"G (1000GeV)\"]->Add(hist_mll[graviton[i]]); \n",
    "                                cout << \"Added DSID \" << graviton[i] << \" to \" << signal_masses[i] << endl; }\n",
    "    if(signal_masses[i] == 2000){ H_mll_signal[\"G (2000GeV)\"]->Add(hist_mll[graviton[i]]); \n",
    "                                cout << \"Added DSID \" << graviton[i] << \" to \" << signal_masses[i] << endl; }\n",
    "    if(signal_masses[i] == 3000){ H_mll_signal[\"G (3000GeV)\"]->Add(hist_mll[graviton[i]]);\n",
    "                                 cout << \"Added DSID \" << graviton[i] << \" to \" << signal_masses[i] << endl; }\n",
    "    if(signal_masses[i] == 4000){ H_mll_signal[\"G (4000GeV)\"]->Add(hist_mll[graviton[i]]);\n",
    "                                 cout << \"Added DSID \" << graviton[i] << \" to \" << signal_masses[i] << endl; }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Making new map containing the colours wanted for each background process, \n",
    "// and set the colours of the histogram\n",
    "\n",
    "map<TString, Int_t> colors_signal;\n",
    "\n",
    "colors_signal[\"G (750GeV)\"] = 616+2; //kMagenta;\n",
    "colors_signal[\"G (1000GeV)\"] = 600+1; //kBlue;\n",
    "colors_signal[\"G (2000GeV)\"] = 632+1; //kRed;\n",
    "colors_signal[\"G (3000GeV)\"] = 416+1; //kGreen;\n",
    "colors_signal[\"G (4000GeV)\"] = 900+6; //kPink\n",
    "\n",
    "for(const auto h:Signals){\n",
    "//    H_mll_signal[h]->SetFillColor(colors_signal[h]); \n",
    "//    H_met_signal[h]->SetFillColor(colors_signal[h]);\n",
    "//    H_lep_pt_signal[h]->SetFillColor(colors_signal[h]);\n",
    "    \n",
    "    H_mll_signal[h]->SetLineColor(colors_signal[h]); \n",
    "    H_met_signal[h]->SetLineColor(colors_signal[h]);\n",
    "    H_lep_pt_signal[h]->SetLineColor(colors_signal[h]);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting histograms\n",
    "Plot $m_{ll}$, $E_T^{miss}$ and $p_T$ histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gStyle->SetLegendBorderSize(0); // Remove (default) border around legend\n",
    "TLegend *leg = new TLegend(0.65, 0.60, 0.9, 0.85);\n",
    "TLegend *leg_ = new TLegend(0.65, 0.60, 0.9, 0.85);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg->Clear();\n",
    "leg_->Clear();\n",
    "for (const auto i:Signals){\n",
    "    leg->AddEntry(H_mll_signal[i], i, \"f\");\n",
    "}\n",
    "// leg->AddEntry(hist_mll_sb, \"Graviton+background\", \"f\");\n",
    "\n",
    "// Add your histograms to the legend\n",
    "for(const auto i:Backgrounds){\n",
    "    leg->AddEntry(H_mll[i], i, \"f\");  \n",
    "    leg_->AddEntry(H_mll[i], i, \"f\");\n",
    "}\n",
    "leg->AddEntry(hist_mll_d, \"Data\", \"lep\");\n",
    "leg_->AddEntry(hist_mll_d, \"Data\", \"lep\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Float_t r = 0.3;\n",
    "Float_t epsilon = 0.02;\n",
    "\n",
    "TCanvas *C = new TCanvas(\"c\", \"c\", 600, 600);\n",
    "\n",
    "TPad *p1 = new TPad(\"p1\", \"\", 0, 0.25, 1, 1);\n",
    "TPad *p2 = new TPad(\"p2\", \"\", 0, 0.0, 1, 0.25);\n",
    "\n",
    "TPad *p3 = new TPad(\"p3\", \"\", 0, 0.25, 1, 1);\n",
    "TPad *p4 = new TPad(\"p4\", \"\", 0, 0.0, 1, 0.25);\n",
    "\n",
    "TPad *p5 = new TPad(\"p5\", \"\", 0, 0.25, 1, 1);\n",
    "TPad *p6 = new TPad(\"p6\", \"\", 0, 0.0, 1, 0.25);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C->SetTicks(1, 1);\n",
    "C->SetLeftMargin(0.13);\n",
    "\n",
    "p1->SetLogy();\n",
    "p1->SetBottomMargin(0.03);\n",
    "p1->Draw();\n",
    "p1->cd();\n",
    "gStyle->SetOptStat(0);\n",
    "gStyle->SetOptTitle(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_mll_d->SetLineColor(kBlack);\n",
    "hist_mll_d->SetMarkerStyle(kFullCircle);\n",
    "hist_mll_d->SetMarkerColor(kBlack);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are less statistics, then it is more likely that the MC samples and real data will differ as \n",
    "MC predicts the amount of events one can expect at a certain range of e.g. m_ll. As long as the number of \n",
    "events of MC and real data do not differ too much at high statistics, e.g. 10^6 events,\n",
    "then MC is correct. If they do, then there is something wrong with the theory modelling the MC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_mll->Draw(\"hist\");\n",
    "stack_mll->SetMaximum(5E4);     //set maximum range on axis\n",
    "stack_mll->SetMinimum(1e-2);\n",
    "stack_mll->GetYaxis()->SetTitle(\"# events\");\n",
    "stack_mll->GetYaxis()->SetTitleOffset(1.3);\n",
    "stack_mll->GetXaxis()->SetTitle(\"m_{ll} (GeV)\");\n",
    "stack_mll->GetXaxis()->SetTitleOffset(1.3);\n",
    "stack_mll->GetXaxis()->SetLabelSize(0);\n",
    "\n",
    "/*\n",
    "for (int i=0; i<Signals.size(); i++){\n",
    "    if (i==0){\n",
    "        H_mll_signal[Signals[i]]->Draw(\"hist\");\n",
    "        H_mll_signal[Signals[i]]->SetMaximum(1E5);     //set maximum range on axis\n",
    "        H_mll_signal[Signals[i]]->SetMinimum(1e-3);\n",
    "        H_mll_signal[Signals[i]]->GetYaxis()->SetTitle(\"# events\");\n",
    "        H_mll_signal[Signals[i]]->GetYaxis()->SetTitleOffset(1.3);\n",
    "        H_mll_signal[Signals[i]]->GetXaxis()->SetTitle(\"m_{ll} (GeV)\");\n",
    "        H_mll_signal[Signals[i]]->GetXaxis()->SetTitleOffset(1.3);\n",
    "        H_mll_signal[Signals[i]]->GetXaxis()->SetLabelSize(0);\n",
    "    }\n",
    "    else { H_mll_signal[Signals[i]]->Draw(\"same hist\");}\n",
    "}\n",
    "\n",
    "\n",
    "*/\n",
    "for (int i=0; i<Signals.size(); i++){\n",
    "    H_mll_signal[Signals[i]]->Draw(\"same hist\");\n",
    "}\n",
    "hist_mll_d->Draw(\"same E\");\n",
    "leg->Draw();\n",
    "\n",
    "p1->Update();\n",
    "p1->RedrawAxis();\n",
    "\n",
    "C->cd();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2->Draw();\n",
    "p2->cd();\n",
    "p2->SetGridy();\n",
    "\n",
    "hist_mll_r->SetMaximum(1.99);\n",
    "hist_mll_r->SetMinimum(0.01);\n",
    "hist_mll_r->GetYaxis()->SetTitle(\"Data/pred.\");\n",
    "hist_mll_r->GetYaxis()->SetTitleSize(0.07);\n",
    "hist_mll_r->GetYaxis()->SetLabelSize(0.07);\n",
    "hist_mll_r->GetXaxis()->SetTitle(\"m_{ll} (GeV)\");\n",
    "hist_mll_r->GetXaxis()->SetTitleSize(0.11);\n",
    "hist_mll_r->GetXaxis()->SetLabelSize(0.08);\n",
    "hist_mll_r->SetMarkerStyle(kFullCircle);\n",
    "hist_mll_r->SetMarkerColor(kBlack);\n",
    "hist_mll_r->Draw(\"0PZ\");\n",
    "//hist_mll_r->GetYaxis()->SetTitle(\"Data/pred.\");\n",
    "\n",
    "p2->SetTopMargin(0.05);\n",
    "p2->SetBottomMargin(0.25); \n",
    "p2->SetLeftMargin(0.10);\n",
    "p2->SetTickx();\n",
    "p2->Update();\n",
    "p2->RedrawAxis();\n",
    "\n",
    "C->cd();\n",
    "C->Update(); \n",
    "C->Draw();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_met_d->SetLineColor(kBlack);\n",
    "hist_met_d->SetMarkerStyle(kFullCircle);\n",
    "hist_met_d->SetMarkerColor(kBlack);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_met_sb->SetFillColor(860);  //kAzure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C->SetTicks(1, 1);\n",
    "C->SetLeftMargin(0.13);\n",
    "\n",
    "p3->SetLogy();\n",
    "p3->SetBottomMargin(0.03);\n",
    "p3->Draw();\n",
    "p3->cd();\n",
    "stack_met->Draw(\"hist\");\n",
    "stack_met->SetMaximum(1E8);     //set maximum range on axis\n",
    "stack_met->SetMinimum(1e-3);\n",
    "stack_met->GetYaxis()->SetTitle(\"# events\");\n",
    "stack_met->GetYaxis()->SetTitleOffset(1.3);\n",
    "stack_met->GetXaxis()->SetTitle(\"E_{T}^{miss} (GeV)\");\n",
    "stack_met->GetXaxis()->SetTitleOffset(1.3);\n",
    "stack_met->GetXaxis()->SetLabelSize(0);\n",
    "hist_met_d->Draw(\"same e\");\n",
    "leg_->Draw();\n",
    "\n",
    "p3->Update();\n",
    "p3->RedrawAxis();\n",
    "\n",
    "C->cd();\n",
    "/*\n",
    "Often more difficult to model E_miss\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4->Draw();\n",
    "p4->cd();\n",
    "p4->SetGridy();\n",
    "\n",
    "hist_met_r->SetMaximum(1.99);\n",
    "hist_met_r->SetMinimum(0.01);\n",
    "hist_met_r->GetYaxis()->SetTitle(\"Data/pred.\");\n",
    "hist_met_r->GetYaxis()->SetTitleSize(0.07);\n",
    "hist_met_r->GetYaxis()->SetLabelSize(0.07);\n",
    "hist_met_r->GetXaxis()->SetTitle(\"E_{T}^{miss} (GeV)\");\n",
    "hist_met_r->GetXaxis()->SetTitleSize(0.11);\n",
    "hist_met_r->GetXaxis()->SetLabelSize(0.055);\n",
    "hist_met_r->SetMarkerStyle(kFullCircle);\n",
    "hist_met_r->SetMarkerColor(kBlack);\n",
    "hist_met_r->Draw(\"0PZ\");\n",
    "//hist_mll_r->GetYaxis()->SetTitle(\"Data/pred.\");\n",
    "\n",
    "p4->SetTopMargin(0.05);\n",
    "p4->SetBottomMargin(0.25); \n",
    "p4->Update();\n",
    "p4->RedrawAxis(); \n",
    "\n",
    "C->cd();\n",
    "C->Update(); \n",
    "C->Draw();\n",
    "//C_->Print(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_lep_pt_d->SetLineColor(kBlack); \n",
    "hist_lep_pt_d->SetMarkerStyle(kFullCircle); \n",
    "hist_lep_pt_d->SetMarkerColor(kBlack);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_lep_pt_sb->SetFillColor(860); //kAzure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C->SetTicks(1, 1);\n",
    "C->SetLeftMargin(0.13);\n",
    "\n",
    "p5->SetLogy();\n",
    "p5->SetBottomMargin(0.03);\n",
    "p5->Draw();\n",
    "p5->cd();\n",
    "\n",
    "stack_lep_pt->Draw(\"hist\");\n",
    "stack_lep_pt->SetMaximum(1E8);     //set maximum range on axis\n",
    "stack_lep_pt->SetMinimum(1e-3);\n",
    "stack_lep_pt->GetYaxis()->SetTitle(\"# events\");\n",
    "stack_lep_pt->GetYaxis()->SetTitleOffset(1.3);\n",
    "stack_lep_pt->GetXaxis()->SetTitle(\"p_{T} (GeV)\");\n",
    "stack_lep_pt->GetXaxis()->SetTitleOffset(1.3);\n",
    "stack_lep_pt->GetXaxis()->SetLabelSize(0);\n",
    "hist_lep_pt_d->Draw(\"same e\"); \n",
    "leg_->Draw();\n",
    "\n",
    "p5->Update();\n",
    "p5->RedrawAxis();\n",
    "\n",
    "C->cd();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p6->Draw();\n",
    "p6->cd();\n",
    "p6->SetGridy();\n",
    "\n",
    "hist_pt_r->SetMaximum(1.99);\n",
    "hist_pt_r->SetMinimum(0.01);\n",
    "hist_pt_r->GetYaxis()->SetTitle(\"Data/pred.\");\n",
    "hist_pt_r->GetYaxis()->SetTitleSize(0.07);\n",
    "hist_pt_r->GetYaxis()->SetLabelSize(0.07);\n",
    "hist_pt_r->GetXaxis()->SetTitle(\"p_{T} (GeV)\");\n",
    "hist_pt_r->GetXaxis()->SetTitleSize(0.11);\n",
    "hist_pt_r->GetXaxis()->SetLabelSize(0.055);\n",
    "hist_pt_r->SetMarkerStyle(kFullCircle);\n",
    "hist_pt_r->SetMarkerColor(kBlack);\n",
    "hist_pt_r->SetTitle(\"\");\n",
    "hist_pt_r->Draw(\"0PZ\");\n",
    "\n",
    "p6->SetTopMargin(0.05);\n",
    "p6->SetBottomMargin(0.25);\n",
    "p6->Update();\n",
    "p6->RedrawAxis(); \n",
    "\n",
    "C->cd();\n",
    "C->Update(); \n",
    "C->Draw();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ROOT C++",
   "language": "c++",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".C",
   "mimetype": " text/x-c++src",
   "name": "c++"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
